{% set name = "distance" %}
{% set version = "0.1.3" %}
{% set file_ext = "tar.gz" %}
{% set hash_type = "sha256" %}
{% set hash_value = "60807584f5b6003f5c521aa73f39f51f631de3be5cccc5a1d67166fcbf0d4551" %}

package:
  name: '{{ name|lower }}'
  version: '{{ version }}'

source:
  fn: '{{ name }}-{{ version }}.{{ file_ext }}'
  url: https://files.pythonhosted.org/packages/5c/1a/883e47df323437aefa0d0a92ccfb38895d9416bd0b56262c2e46a47767b8/{{ name | title }}-{{ version }}.{{ file_ext }}
  '{{ hash_type }}': '{{ hash_value }}'

build:
  number: 0
  script: 'python setup.py install '

requirements:
  build:
    - python
  run:
    - python

test:
  imports:
    - distance

about:
  home: https://github.com/doukremt/distance
  license: GNU General Public License (GPL)
  license_family: LGPL
  license_file: ''
  summary: Utilities for comparing sequences
  description: "distance - Utilities for comparing sequences\n============================================\n\nThis package provides helpers for computing similarities between arbitrary sequences. Included\
    \ metrics are Levenshtein, Hamming, Jaccard, and Sorensen distance, plus some bonuses. All distance computations are implemented in pure Python, and most of them are also implemented in C.\n\n\nInstallation\n\
    ------------\n\nIf you don't want or need to use the C extension, just unpack the archive and run, as root:\n\n\t# python setup.py install\n\nFor the C extension to work, you need the Python source\
    \ files, and a C compiler (typically Microsoft Visual C++ 2010 on Windows, and GCC on Mac and Linux). On a Debian-like system, you can get all of these with:\n\n\t# apt-get install gcc pythonX.X-dev\n\
    \nwhere X.X is the number of your Python version.\n\nThen you should type:\n\n\t# python setup.py install --with-c\n\nNote the use of the `--with-c` switch.\n\n\nUsage\n-----\n\nA common use case for\
    \ this module is to compare single words for similarity:\n\n\t>>> distance.levenshtein(\"lenvestein\", \"levenshtein\")\n\t3\n\t>>> distance.hamming(\"hamming\", \"hamning\")\n\t1\n\nIf there is not\
    \ a one-to-one mapping between sounds and glyphs in your language, or if you want to compare not glyphs, but syllables or phonems, you can pass in tuples of characters:\n\n\t>>> t1 = (\"de\", \"ci\"\
    , \"si\", \"ve\")\n\t>>> t2 = (\"de\", \"ri\", \"si\", \"ve\")\n\t>>> distance.levenshtein(t1, t2)\n\t1\n\nComparing lists of strings can also be useful for computing similarities between sentences,\
    \ paragraphs, etc.:\n\n\t>>> sent1 = ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n\t>>> sent2 = ['the', 'lazy', 'fox', 'jumps', 'over', 'the', 'crazy', 'dog']\n\t>>> distance.levenshtein(sent1,\
    \ sent2)\n\t3\n\nHamming and Levenshtein distance can be normalized, so that the results of several distance measures can be meaningfully compared. Two strategies are available for Levenshtein: either\
    \ the length of the shortest alignment between the sequences is taken as factor, or the length of the longer one. Example uses:\n\n\t>>> distance.hamming(\"fat\", \"cat\", normalized=True)\n\t0.3333333333333333\n\
    \t>>> distance.nlevenshtein(\"abc\", \"acd\", method=1)  # shortest alignment\n\t0.6666666666666666\n\t>>> distance.nlevenshtein(\"abc\", \"acd\", method=2)  # longest alignment\n\t0.5\n\n`jaccard`\
    \ and `sorensen` return a normalized value per default:\n\n\t>>> distance.sorensen(\"decide\", \"resize\")\n\t0.5555555555555556\n\t>>> distance.jaccard(\"decide\", \"resize\")\n\t0.7142857142857143\n\
    \nAs for the bonuses, there is a `fast_comp` function, which computes the distance between two strings up to a value of 2 included. If the distance between the strings is higher than that, -1 is returned.\
    \ This function is of limited use, but on the other hand it is quite faster than `levenshtein`. There is also a `lcsubstrings` function which can be used to find the longest common substrings in two\
    \ sequences.\n\nFinally, two convenience iterators `ilevenshtein` and `ifast_comp` are provided, which are intended to be used for filtering from a long list of sequences the ones that are close to\
    \ a reference one. They both return a series of tuples (distance, sequence). Example:\n\n\t>>> tokens = [\"fo\", \"bar\", \"foob\", \"foo\", \"fooba\", \"foobar\"]\n\t>>> sorted(distance.ifast_comp(\"\
    foo\", tokens))\n\t[(0, 'foo'), (1, 'fo'), (1, 'foob'), (2, 'fooba')]\n\t>>> sorted(distance.ilevenshtein(\"foo\", tokens, max_dist=1))\n\t[(0, 'foo'), (1, 'fo'), (1, 'foob')]\n\n`ifast_comp` is particularly\
    \ efficient, and can handle 1 million tokens without a problem.\n\nFor more informations, see the functions documentation (`help(funcname)`).\n\nHave fun!\n\n\nChangelog\n---------\n\n20/11/13:\n* Switched\
    \ back to using the to-be-deprecated Python unicode api. Good news is that this makes the\nC extension compatible with Python 2.7+, and that distance computations on unicode strings is now\nmuch faster.\n\
    * Added a C version of `lcsubstrings`.\n* Added a new method for computing normalized Levenshtein distance.\n* Added some tests.\n\n12/11/13:\nExpanded `fast_comp` (formerly `quick_levenshtein`) so\
    \ that it can handle transpositions.\nFixed variable interversions in (C) `levenshtein` which produced sometimes strange results.\n\n10/11/13:\nAdded `quick_levenshtein` and `iquick_levenshtein`.\n\n\
    05/11/13:\nAdded Sorensen and Jaccard metrics, fixed memory issue in Levenshtein."
  doc_url: ''
  dev_url: ''

extra:
  recipe-maintainers: ''
